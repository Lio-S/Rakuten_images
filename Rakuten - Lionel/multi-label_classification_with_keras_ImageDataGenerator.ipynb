{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utiliser un outil de gestion des versions de Python (`pyenv`)**\n",
    "\n",
    "`pyenv` permet de gérer plusieurs versions de Python facilement :\n",
    "\n",
    "1. Installez `pyenv` :\n",
    "   ```bash\n",
    "   curl https://pyenv.run | bash\n",
    "   ```\n",
    "\n",
    "2. Ajoutez `pyenv` à votre environnement :\n",
    "   ```bash\n",
    "   export PYENV_ROOT=\"$HOME/.pyenv\"\n",
    "   [[ -d $PYENV_ROOT/bin ]] && export PATH=\"$PYENV_ROOT/bin:$PATH\"\n",
    "   eval \"$(pyenv init -)\"\"\n",
    "   ```\n",
    "   \n",
    "3. Installez Python 3.6.15 avec `pyenv` :\n",
    "   ```bash\n",
    "   pyenv install 3.6.15\n",
    "   ```\n",
    "\n",
    "#si pb 3.1 et 2\\\n",
    "3.1 Installer les dépendances requise :\n",
    "   ```bash\n",
    "   sudo apt update\n",
    "   sudo apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev \\\n",
    "      libssl-dev libreadline-dev libffi-dev curl libbz2-dev libsqlite3-dev tk-dev liblzma-dev\n",
    "   ```\n",
    "\n",
    "3.2 Recompiler Python 3.6.15 :\n",
    "   ```bash\n",
    "   pyenv uninstall 3.6.15\n",
    "   pyenv install 3.6.15\n",
    "   ```\n",
    "\n",
    "4. Utilisez cette version pour créer un environnement virtuel :\n",
    "   ```bash\n",
    "   pyenv virtualenv 3.6.15 tf_env_3_6\n",
    "   pyenv activate tf_env_3_6\n",
    "   ```\n",
    "\n",
    "5. Vérifiez la version de Python :\n",
    "   ```bash\n",
    "   python --version\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommandation**\n",
    "\n",
    "Si vous avez besoin de TensorFlow 1.x ou de dépendances spécifiques comme `keras-preprocessing==1.0.8`, la **solution 3 (pyenv)** est la plus propre pour gérer plusieurs versions de Python sans perturber votre système. Si vous préférez ne pas compiler ou utiliser `pyenv`, vous pouvez envisager une alternative comme **Docker** pour un environnement isolé.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet50\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet50\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (f1_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             multilabel_confusion_matrix)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_labels(xml_data, unique_labels):\n",
    "    root = ET.XML(xml_data)\n",
    "    labels = set() if unique_labels else []\n",
    "    labels_add = labels.add if unique_labels else labels.append\n",
    "    for i, child in enumerate(root):\n",
    "        if child.tag == 'filename':\n",
    "            img_filename = child.text\n",
    "        if child.tag == 'object':\n",
    "            for subchild in child:\n",
    "                if subchild.tag == 'name':\n",
    "                    labels_add(subchild.text)\n",
    "    return img_filename, list(labels)\n",
    "\n",
    "def get_labels(annotations_dir, unique_labels=True):\n",
    "    for annotation_file in annotations_dir.iterdir():\n",
    "        with open(annotation_file) as f:\n",
    "            yield xml_to_labels(f.read(), unique_labels)\n",
    "            \n",
    "def plot_confusion_matrix(cm, classes, title, ax):\n",
    "\n",
    "    ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks), ax.xaxis.set_ticklabels(classes)\n",
    "    ax.set_yticks(tick_marks), ax.yaxis.set_ticklabels(classes)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Truth')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "    \n",
    "def plot_multiclass_confusion_matrix(y_true, y_pred, label_to_class, save_plot=False):\n",
    "    fig, axes = plt.subplots(int(np.ceil(len(label_to_class) / 2)), 2, figsize=(15, 60))\n",
    "    axes = axes.flatten()\n",
    "    for i, conf_matrix in enumerate(multilabel_confusion_matrix(y_true, y_pred)):\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "        f1 = 2 * tp / (2 * tp + fp + fn + sys.float_info.epsilon)\n",
    "        recall = tp / (tp + fn + sys.float_info.epsilon)\n",
    "        precision = tp / (tp + fp + sys.float_info.epsilon)\n",
    "        plot_confusion_matrix(\n",
    "            np.array([[tp, fn], [fp, tn]]),\n",
    "            classes=['+', '-'],\n",
    "            title=f'Label: {label_to_class[i]}\\nf1={f1:.5f}\\nrecall={recall:.5f}\\nprecision={precision:.5f}',\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "    if save_plot:\n",
    "        plt.savefig('confusion_matrices.png', dpi=50)\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, validation_generator, validation_steps=None, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.validation_generator = validation_generator\n",
    "        self.validation_steps = validation_steps\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1_scores = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for i in range(self.validation_steps):\n",
    "            X, y = self.validation_generator[i]\n",
    "            y_true.append(y)\n",
    "            predictions = self.model.predict(X, verbose=0)\n",
    "            y_pred.append(predictions)\n",
    "            \n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = (np.vstack(y_pred) > self.threshold).astype('int')\n",
    "        \n",
    "        _val_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        _val_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        _val_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        self.val_f1_scores.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        \n",
    "        print(f\" - val_f1_score: {_val_f1:.5f} - val_precision: {_val_precision:.5f} - val_recall: {_val_recall:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 70)\n",
    "plt.rcParams['figure.figsize'] = (25, 6)\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification with Keras' `ImageDataGenerator`\n",
    "\n",
    "- Author: Rodrigo Agundez\n",
    "- Date: January 28, 2019\n",
    "\n",
    "This notebook demonstrates how to use the `ImageDataGenerator` from `Keras` to solve a multi-label classification problem. That is a classification problem where each image can have multiple class labels.\n",
    "\n",
    "The dataset used for this demonstration is [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/). For classification this dataset contains ~17,000 images and 20 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Note: make sure you are in a virtual environment as this notebook will install the development verions of `keras-preprocessing` and `scikit-learn`.\n",
    "\n",
    "In order to run this notebook you need:\n",
    "\n",
    "    - keras\n",
    "    - keras-preprocessing\n",
    "    - matplotlib\n",
    "    - numpy\n",
    "    - pandas\n",
    "    - Pillow\n",
    "    - seaborn\n",
    "\n",
    "**Update `keras-preprocessing` to the latests version in github.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U keras-preprocessing==1.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update `scikit-learn` to the latests version in github.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/scikit-learn/scikit-learn.git --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports from the updated versions of the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About ImageDataGenerator\n",
    "\n",
    "`ImageDataGenerator` is a known Keras class which has 2 main functionalities:\n",
    "\n",
    " - Data augmentation\n",
    " \n",
    " This is a technique which can reduce the overfitting by increasing the amount of observations in the training data. In the case of images this is done via [affine Transformations](https://homepages.inf.ed.ac.uk/rbf/HIPR2/affine.htm) for example.\n",
    " \n",
    " \n",
    " - Loading image data from disk\n",
    " \n",
    " This allows the user to train over more data than the one that is able to fit in memory.\n",
    "\n",
    "### About `flow_from_directory`\n",
    "\n",
    "The most known use of `ImageDataGenerator` is via its `flow_from_directory` method, which allows you to do several tasks but it's restrcited to the following directory structure of your data:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg)\n",
    "\n",
    "* You can read more about it in this [blog post](https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720) or in the [Keras documentation](https://keras.io/preprocessing/image/).\n",
    "\n",
    "### About `flow_from_dataframe`\n",
    "\n",
    "In order to provide Keras users with a more flexible API and keeping the two main advantages, the method `flow_from_dataframe` was introduced. Under the hood the mechanics are handle by the `DataFrameIterator` class which will cover a wide variety of cases that `flow_from_directory` is not able to do.\n",
    "\n",
    "* You can read more aobut it in this [blog post](https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c)\n",
    "\n",
    "In particular I recently added the functionality to perform `multi-class` classification, and this notebook concentrates on this topic.\n",
    "\n",
    "* This was possible before but in a hacky not very API friendly way. You can read about it [here](https://github.com/keras-team/keras-preprocessing/issues/135)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification\n",
    "\n",
    "A multi-label classification scenario exists when a single observation (images in this example) can have multiple class labels. Let's take an example from the VOC2010 dataset:\n",
    "* Not to be confused with multi-class classification where there are more than 2 classes but each observation has a single class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread(Path('~/.keras/datasets/VOC2012/JPEGImages/2008_007056.jpg').expanduser()))\n",
    "plt.title('Labels: [bus, person, boat]')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting images filenames and labels\n",
    "\n",
    "Download the [VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar) dataset and make sure `Annotations` and `JPEGImages` are under the `VOC2012` directory. Below an example of the folder structure and some file examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/.keras/datasets && tree VOC2012 -l -L 2 -P 201[0-1]_00000[2-3].* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the images can be loaded into the DataFrame as relative or absolute paths. In this example we will use relative paths and then use the directory parameter. The labels can be loaded in a list, tuple or string if only one. Below a sample of the dataframe with the filenames and labels for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = Path('~/.keras/datasets/VOC2012/Annotations').expanduser()\n",
    "images_dir = Path('~/.keras/datasets/VOC2012/JPEGImages').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_metadata = pd.DataFrame(get_labels(annotations_dir), columns=['filename', 'labels'])\n",
    "print(f'Found {len(img_metadata)} images')\n",
    "img_metadata.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the different classes are represented in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create figure with larger size\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Count and sort labels\n",
    "all_labels = [label for lbs in img_metadata['labels'] for label in lbs]\n",
    "labels_count = Counter(all_labels)\n",
    "most_common = labels_count.most_common()\n",
    "\n",
    "# Create plot\n",
    "ax = sns.barplot(x=[k for k,v in most_common], \n",
    "                 y=[v for k,v in most_common])\n",
    "\n",
    "# Customize plot\n",
    "plt.yscale('log')\n",
    "plt.ylim(100, 10000)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of images with a class label')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are imbalanced, we can address that in different ways, for example using sample weights or class weights. For now let's just continue with the functionality.\n",
    "\n",
    "* Sample weights are not implemented in `flow_from_dataframe` yet, but this functionality is expected to be added soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look on how the size of the iamges is distributed accross the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, size = np.empty(len(img_metadata)), np.empty(len(img_metadata)), np.empty(len(img_metadata)) \n",
    "for i, img_filepath in img_metadata['filename'].items():\n",
    "    w, h = Image.open(images_dir.joinpath(img_filepath)).size\n",
    "    width[i], height[i], size[i] = w, h, w * h * 3 * 1E-6\n",
    "plt.scatter(width, height, alpha=0.5)\n",
    "plt.xlabel('Width'); plt.ylabel('Height'); plt.show()\n",
    "plt.hist(size, bins=50, log=True)\n",
    "plt.xlabel('Image size (MB)');\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, the dataset contains images of different heights and widths. I won't go into detail but this is not really a problem if at the end of the feature extraction via Convolutional layers a Global pooling layer is introduced. Unfortunately when using the `flow_from_dataframe` method all images need to be standarized to the same size, this size is defined via the `target_size` parameter.\n",
    "\n",
    "* This is because each batch of images is loaded into a numpy array. This would be a nice new feature to have though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples of the labels and the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = Path('VOC2012/JPEGImages')\n",
    "observation = img_metadata.sample(n=1).to_dict(orient='records')[0]\n",
    "img = plt.imread(images_dir.joinpath(observation['filename']))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(observation['labels']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to instantiate the `ImageDataGenerator`. I'll do this with a simple setup just normalizing the the pixel values. I also included a validation split to use it for validation stats during training after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen = ImageDataGenerator(rescale=1/255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training and validation `DataFrameIterator`, in the case of multi-label classification the `class_mode` should be `categorical` (which is the default value). Since I defined a `validation_split` the validation batches ca be retrieved by specifying the `subset` as `validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Encoder les labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_labels = mlb.fit_transform(img_metadata['labels'])\n",
    "\n",
    "# Créer une nouvelle colonne avec les labels encodés\n",
    "img_metadata['encoded_labels'] = [np.array(x) for x in encoded_labels]\n",
    "\n",
    "img_iter = img_gen.flow_from_dataframe(\n",
    "    img_metadata,\n",
    "    shuffle=True,\n",
    "    directory=images_dir,\n",
    "    x_col='filename',\n",
    "    y_col='encoded_labels',\n",
    "    class_mode='raw',  # Utiliser 'raw' au lieu de 'binary'\n",
    "    target_size=(128, 128),\n",
    "    batch_size=20,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Mapping pour la conversion\n",
    "label_to_class = dict(enumerate(mlb.classes_))\n",
    "\n",
    "img_iter_val = img_gen.flow_from_dataframe(\n",
    "    img_metadata,\n",
    "    shuffle=False,\n",
    "    directory=images_dir,\n",
    "    x_col='filename',\n",
    "    y_col='labels',\n",
    "    class_mode='categorical',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=20,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples of images from the batches being generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_class = dict(enumerate(mlb.classes_))\n",
    "\n",
    "def array_to_labels(onehot_array, label_to_class):\n",
    "    labels = []\n",
    "    idx = np.where(onehot_array == 1)[0]\n",
    "    return [label_to_class[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(img_iter)\n",
    "plt.imshow(np.hstack(images[:3]))\n",
    "plt.title((' ' * 5).join(str(array_to_labels(x, label_to_class)) for x in labels[:3]))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the `ResNet50` pre-trained model in this example. In order to make things work I will replace the last fully connected layer of the network and replace it for one with 20 neuron units, one for each class. \n",
    "\n",
    "* The output layer from `ResNet50` if `include_top=False` has size 2048, I wouldn't normally followed with a fully connected layer of 20 neurons, but for this example is sufficient to show functionality. Normally I try dropping the output units by 1/3 or 1/10 if 1/3 is not sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    base_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=None,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    predictions = Dense(20, activation='sigmoid')(base_model.output)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for training the model, for a multi-class problem we need the `categorical_crossentropy` loss function with `sigmoid` activation function in the last layer. For fitting the modelwe pass the training iterator and the validation iterator for real-time monitoring during training.\n",
    "\n",
    "* In contrast with a multi-class problem where we would use a `softmax` activation function in the last layer, here we use a `sigmoid` function. I won't go to the details but basically the probability of each possible label to appear in the image is taken as independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = sum(labels_count.values())\n",
    "class_weights = {i: total_counts / count for i, (cls, count) in enumerate(labels_count.items())}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "class SimpleGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, directory, x_col, y_col, batch_size=32):\n",
    "        self.df = df.copy()\n",
    "        self.directory = directory\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch = self.df.iloc[batch_indices]\n",
    "        \n",
    "        # Batch réel (pour le dernier batch partiel)\n",
    "        current_batch_size = len(batch)\n",
    "        X = np.zeros((current_batch_size, 128, 128, 3), dtype='float32')\n",
    "        y = np.zeros((current_batch_size, 20), dtype='float32')\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch.iterrows()):\n",
    "            img_path = os.path.join(self.directory, row[self.x_col])\n",
    "            img = load_img(img_path, target_size=(128, 128))\n",
    "            X[i] = img_to_array(img) / 255.0\n",
    "            y[i] = row[self.y_col]\n",
    "        \n",
    "        return tf.convert_to_tensor(X), tf.convert_to_tensor(y)\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self, validation_generator, validation_steps=None, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.validation_generator = validation_generator\n",
    "        self.validation_steps = validation_steps\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1_scores = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for i in range(self.validation_steps):\n",
    "            X, y = self.validation_generator[i]\n",
    "            y_true.append(y)\n",
    "            predictions = self.model.predict(X, verbose=0)\n",
    "            y_pred.append(predictions)\n",
    "            \n",
    "        y_true = np.vstack(y_true)\n",
    "        y_pred = (np.vstack(y_pred) > self.threshold).astype('int')\n",
    "        \n",
    "        _val_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        _val_recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        _val_precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        \n",
    "        self.val_f1_scores.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        \n",
    "        print(f\" - val_f1: {_val_f1:.5f} - val_precision: {_val_precision:.5f} - val_recall: {_val_recall:.5f}\")\n",
    "\n",
    "# Utilisation\n",
    "train_gen = SimpleGenerator(img_metadata, images_dir, 'filename', 'encoded_labels')\n",
    "\n",
    "metrics_callback = Metrics(train_gen, validation_steps=10)\n",
    "\n",
    "model = make_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=1, \n",
    "    steps_per_epoch=1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[metrics_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_batches = 10\n",
    "threshold = 0.5\n",
    "img_iter_val_0, img_iter_val_1 = itertools.tee(img_iter_val, 2)\n",
    "y_true = np.vstack(next(img_iter_val_0)[1] for _ in range(nr_batches)).astype('int')\n",
    "y_pred = (model.predict_generator(img_iter_val_1, steps=nr_batches) > threshold).astype('int')\n",
    "plot_multiclass_confusion_matrix(y_true, y_pred, label_to_class, save_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
