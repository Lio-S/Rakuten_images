{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f017dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 19:11:00,421 - classification_pipeline - INFO - Chargement des features pré-calculées...\n",
      "2025-07-21 19:11:02,517 - classification_pipeline - INFO - Chargement effectué avec succès.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors du chargement des configurations : [Errno 2] No such file or directory: 'data/models/model_configs.yaml'\n",
      "\n",
      "Résultats de la comparaison:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from preprocess import ProductClassificationPipeline, PipelineConfig\n",
    "from utils import generate_prediction_report, analyze_prediction_errors, plot_prediction_distribution\n",
    "from data.processed_data.executer_pour_telecharger_donnees import telecharger_et_extraire_zip\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "url = \"https://drive.google.com/file/d/1guhuHp0dVRPWCtZ7570jEsTub6m2RrRF/view?usp=sharing\"\n",
    "fichier_zip = \"Preprocessed_data.zip\"\n",
    "dossier_donnees_pretraitees = \"data/processed_data\"\n",
    "fichier_donnees_pretraitees = \"data/processed_data/y_train.npz\"\n",
    "\n",
    "\n",
    "def load_model_configs():\n",
    "    \"\"\"\n",
    "    Charge les configurations des modèles depuis le YAML\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configurations des modèles avec leurs paramètres\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_path = os.path.join('data', 'models', 'model_configs.yaml')\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            configs = yaml.safe_load(f)\n",
    "        \n",
    "        # Vérification de la présence de tous les modèles attendus\n",
    "        expected_models = {'xgboost', 'neural_net'}\n",
    "        missing_models = expected_models - set(configs.keys())\n",
    "        \n",
    "        if missing_models:\n",
    "            print(f\"Attention : modèles manquants dans la configuration : {missing_models}\")\n",
    "            \n",
    "        return configs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des configurations : {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Forcer le prétraitement des données ou les télécharger si souhaité\n",
    "    force_preprocess = False # Mettre à True pour forcer le preprocesing des données\n",
    "    \n",
    "    if not force_preprocess and not os.path.exists(fichier_donnees_pretraitees):        \n",
    "        # Appel de la fonction pour télécharger + extraire\n",
    "        telecharger_et_extraire_zip(\n",
    "            url=url,\n",
    "            fichier_zip=fichier_zip,\n",
    "            dossier_extraction=dossier_donnees_pretraitees\n",
    "        )\n",
    "        \n",
    "    # Chargement des configurations\n",
    "    config = PipelineConfig.from_yaml('config.yaml')\n",
    "    pipeline = ProductClassificationPipeline(config)\n",
    "    \n",
    "    #Prétraitement\n",
    "    try:\n",
    "        pipeline.prepare_data(force_preprocess_image=False, force_preprocess_text=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du prétraitement : {str(e)}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Chargement des configurations des modèles depuis le YAML\n",
    "    models_to_test = load_model_configs()\n",
    "    \n",
    "    # Initialisation des dictionnaires pour stocker tous les résultats\n",
    "    all_results = {}\n",
    "    \n",
    "    evalute = True #Forcer l'évaluation (Maj des résultats)\n",
    "    \n",
    "    for model_name, params in models_to_test.items():\n",
    "        print(f\"\\nTraitement  de {model_name}\")\n",
    "        recherche_hyperparametres = False\n",
    "        if recherche_hyperparametres:\n",
    "            print(f\"\\nRecherche des hyperparamètres pour {model_name}\")            \n",
    "            # Optimisation des hyperparamètres\n",
    "            if model_name == 'xgboost':\n",
    "                best_params = pipeline.optimize_hyperparameters('xgboost')\n",
    "                pipeline.save_hyperparameters('xgboost', best_params)\n",
    "                params.update(best_params)\n",
    "            elif model_name == 'neural_net':\n",
    "                best_params = pipeline.optimize_hyperparameters('neural_net')\n",
    "                pipeline.save_hyperparameters('neural_net', best_params)\n",
    "                params.update(best_params)\n",
    "    \n",
    "        # Vérification des prédictions existantes\n",
    "        if pipeline.predictions_exist(model_name):\n",
    "            predictions, probabilities = pipeline.load_predictions(model_name)\n",
    "            pipeline.load_model(model_name)\n",
    "            print(f\"Prédictions chargées pour {model_name}\")\n",
    "        else:\n",
    "            # Chargement ou entraînement du modèle\n",
    "            model_dir = os.path.join(pipeline.config.model_path, model_name)\n",
    "            if os.path.exists(model_dir):\n",
    "                try:\n",
    "                    pipeline.load_model(model_name)\n",
    "                    print(f\"Modèle {model_name} chargé\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur chargement {model_name}: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                pipeline.train_model(model_type=model_name, **params)\n",
    "\n",
    "            # Prévisions\n",
    "            predictions, probabilities = pipeline.predict(pipeline.preprocessed_data['X_test_split'])\n",
    "                        \n",
    "            # Création du DataFrame de sortie\n",
    "            pipeline.save_predictions(model_name, predictions, probabilities)\n",
    "\n",
    "        file_path_rapport = os.path.join('data', 'rapports', f'rapport_{model_name}.csv')\n",
    "        file_path_erreurs = os.path.join('data', 'erreurs', f'erreurs_{model_name}.csv')\n",
    "        if os.path.exists(file_path_rapport):\n",
    "            print(f\"Le rapport {model_name} existe.\")\n",
    "        if os.path.exists(file_path_erreurs):\n",
    "            print(f\"Le fichier erreurs {model_name} existe.\")\n",
    "            \n",
    "        plot = False   #Pour ne pas afficher le graphe de distribution des prédictions déjà généré\n",
    "        if not os.path.exists(file_path_erreurs) or not os.path.exists(file_path_rapport) or evalute==True:    \n",
    "            # Prévisions et Évaluation\n",
    "            all_results[model_name] = pipeline.evaluate()\n",
    "            plot= True\n",
    "            print(f\"\\nRésultats pour le modèle {model_name}:\\n {all_results[model_name]}\")\n",
    "\n",
    "        # Création du DataFrame de résultats\n",
    "        results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
    "        \n",
    "        # Création du dossier results s'il n'existe pas\n",
    "        results_dir = os.path.join('data', 'results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Sauvegarde des résultats en CSV\n",
    "        results_path = os.path.join(results_dir, 'models_comparaison_results.csv')\n",
    "        results_df.to_csv(results_path)\n",
    "        \n",
    "        # Génération des rapports pour chaque modèle\n",
    "        if not os.path.exists(file_path_rapport):\n",
    "            rapport = generate_prediction_report(pipeline, predictions, probabilities)\n",
    "            rapport.to_csv(file_path_rapport, index=False)\n",
    "            # Visualisation\n",
    "            plot_prediction_distribution(rapport, model_name)\n",
    "        if not os.path.exists(file_path_erreurs):\n",
    "            erreurs = analyze_prediction_errors(\n",
    "                pipeline,\n",
    "                predictions,\n",
    "                pipeline.preprocessed_data['y_test_split']\n",
    "            )\n",
    "            erreurs.to_csv(file_path_erreurs, index=False)\n",
    "            \n",
    "    if evalute:\n",
    "        # Affichage des résultats\n",
    "        print(\"\\nRésultats de la comparaison:\")\n",
    "        print(\"-\" * 50)\n",
    "        for model_name, metrics in all_results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"{metric_name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rakuten (Python 3.10.12)",
   "language": "python",
   "name": "rakuten-3.10.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
