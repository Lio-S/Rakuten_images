# model_configs.yaml
xgboost:
  max_depth: 9
  learning_rate: 0.08
  n_estimators: 300
  min_child_weight: 4
  gamma: 0.12
  subsample: 0.8
  colsample_bytree: 0.8
  reg_alpha: 0.2
  reg_lambda: 1.8
  objective: multi:softprob
  num_class: 27
  tree_method: hist
  max_bin: 256
  device: cuda
  n_jobs: 7
  seed: 42
  eval_metric: mlogloss

# lightgbm:
#   max_depth: 9
#   learning_rate: 0.08
#   n_estimators: 350
#   num_leaves: 96
#   min_data_in_leaf: 15
#   feature_fraction: 0.85
#   lambda_l1: 0.15
#   lambda_l2: 1.2
#   boosting_type: goss
#   min_gain_to_split: 0.1
#   objective: multiclass
#   num_class: 27
#   metric: [multi_error, multi_logloss]
#   device: cpu
#   n_jobs: 7
#   random_state: 42

# catboost:
#   iterations: 300
#   learning_rate: 0.07
#   depth: 9
#   l2_leaf_reg: 2.5
#   bootstrap_type: Bernoulli
#   subsample: 0.82
#   colsample_bylevel: 0.8
#   min_data_in_leaf: 12
#   loss_function: MultiClass
#   eval_metric: TotalF1
#   leaf_estimation_method: Newton
#   random_strength: 0.8
#   thread_count: 7
#   random_seed: 42

neural_net:
  batch_size: 48
  learning_rate: 0.0003
  epochs: 60
  dropout_rate: 0.35
  early_stopping_patience: 7
  weight_decay: 0.015
  optimizer_params:
    betas: [0.9, 0.999]
    eps: 1e-8
  scheduler_params:
    factor: 0.15
    patience: 5
    min_lr: 5e-7
  dataloader_params:
    num_workers: 7
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true

